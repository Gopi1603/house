


get_ipython().getoutput("pip -q install ucimlrepo joblib")
import os, json, math, warnings
warnings.filterwarnings("ignore")

import numpy as np
import pandas as pd

import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

import joblib

# TensorFlow / Keras
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

print("âœ… Imports done")
print("TensorFlow:", tf.__version__)
print("GPU Available:", len(tf.config.list_physical_devices('GPU')) > 0)



from ucimlrepo import fetch_ucirepo

# fetch dataset
dataset = fetch_ucirepo(id=235)

# UCI repo returns features & (sometimes) targets; this dataset is basically all features
df = dataset.data.features.copy()

print("âœ… Raw loaded shape:", df.shape)
display(df.head(5))



print("=== METADATA ===")
print(dataset.metadata)

print("\n=== VARIABLES ===")
display(dataset.variables)



# Combine Date + Time into a single datetime column
# Date format: dd/mm/yyyy, Time format: hh:mm:ss
df["datetime"] = pd.to_datetime(
    df["Date"].astype(str) + " " + df["Time"].astype(str),
    format="%d/%m/%Y %H:%M:%S",
    errors="coerce"
)

# Drop rows where datetime couldn't be parsed (should be extremely rare)
df = df.dropna(subset=["datetime"]).copy()

# Set datetime index and sort
df = df.set_index("datetime").sort_index()

# Drop original Date/Time columns
df = df.drop(columns=["Date", "Time"], errors="ignore")

# Convert all remaining columns to numeric (dataset may load as strings)
for col in df.columns:
    df[col] = pd.to_numeric(df[col], errors="coerce")

print("âœ… After datetime + numeric conversion:", df.shape)
print("Index range:", df.index.min(), "â†’", df.index.max())
display(df.head(5))



missing = df.isna().sum().sort_values(ascending=False)
print("âœ… Missing values per column:")
display(missing[missing > 0])

print("Total missing cells:", int(df.isna().sum().sum()))
print("Total rows:", len(df))






# Missing value handling
before_missing = int(df.isna().sum().sum())
print("Missing cells BEFORE:", before_missing)

df = df.ffill().bfill()

after_missing = int(df.isna().sum().sum())
print("Missing cells AFTER:", after_missing)

# sanity check
assert after_missing == 0, "âŒ Missing values still exist after filling."
print("âœ… Missing values handled successfully.")






# Hourly resampling (mean)
df_hourly = df.resample("H").mean()

print("âœ… Hourly shape:", df_hourly.shape)
print("Hourly index range:", df_hourly.index.min(), "â†’", df_hourly.index.max())
display(df_hourly.head(5))

# check missing introduced by resampling (rare)
print("Missing after resample:", int(df_hourly.isna().sum().sum()))
df_hourly = df_hourly.ffill().bfill()
assert int(df_hourly.isna().sum().sum()) == 0
print("âœ… Hourly dataset ready.")






get_ipython().getoutput("pip -q install minepy")

from minepy import MINE

target_col = "Global_active_power"

# Features candidates = everything except target
feature_cols = [c for c in df_hourly.columns if c != target_col]

# Compute MIC
mic_scores = {}
mine = MINE(alpha=0.6, c=15)

y = df_hourly[target_col].values

for col in feature_cols:
    x = df_hourly[col].values
    mine.compute_score(x, y)
    mic_scores[col] = mine.mic()

mic_df = pd.DataFrame({"feature": list(mic_scores.keys()), "MIC": list(mic_scores.values())})
mic_df = mic_df.sort_values("MIC", ascending=False).reset_index(drop=True)

print("âœ… MIC scores (high â†’ important):")
display(mic_df)

# Choose best features (keep all > 0.1, or at least top 5)
selected = mic_df[mic_df["MIC"] >= 0.10]["feature"].tolist()
if len(selected) < 5:
    selected = mic_df.head(5)["feature"].tolist()

print("âœ… Selected features:", selected)

# Save selected feature list (order is important!)
os.makedirs("/kaggle/working/artifacts", exist_ok=True)
with open("/kaggle/working/artifacts/selected_features.json", "w") as f:
    json.dump(selected, f, indent=2)

print("âœ… Saved: /kaggle/working/artifacts/selected_features.json")






from sklearn.feature_selection import mutual_info_regression

target_col = "Global_active_power"
feature_cols = [c for c in df_hourly.columns if c != target_col]

X_mi = df_hourly[feature_cols].values
y_mi = df_hourly[target_col].values

# Mutual information regression (non-linear)
mi = mutual_info_regression(X_mi, y_mi, random_state=42)

mi_df = pd.DataFrame({"feature": feature_cols, "MI": mi})
mi_df = mi_df.sort_values("MI", ascending=False).reset_index(drop=True)

print("âœ… Mutual Information scores (high â†’ important):")
display(mi_df)

# Select features using threshold (>= 10% of max) OR at least top 5
max_mi = mi_df["MI"].max()
selected = mi_df[mi_df["MI"] >= 0.10 * max_mi]["feature"].tolist()
if len(selected) < 5:
    selected = mi_df.head(5)["feature"].tolist()

print("âœ… Selected features:", selected)

# Save selected features
os.makedirs("/kaggle/working/artifacts", exist_ok=True)
with open("/kaggle/working/artifacts/selected_features.json", "w") as f:
    json.dump(selected, f, indent=2)

print("âœ… Saved: /kaggle/working/artifacts/selected_features.json")






target_col = "Global_active_power"

# Use selected features from saved file (ensures consistency)
with open("/kaggle/working/artifacts/selected_features.json", "r") as f:
    selected_features = json.load(f)

all_cols = selected_features + [target_col]
data = df_hourly[all_cols].copy()

# Time-based split indices
n = len(data)
train_end = int(0.70 * n)
val_end   = int(0.85 * n)

train_df = data.iloc[:train_end]
val_df   = data.iloc[train_end:val_end]
test_df  = data.iloc[val_end:]

print("âœ… Splits:")
print("Train:", train_df.shape, "Val:", val_df.shape, "Test:", test_df.shape)

# Fit scaler only on TRAIN
scaler = MinMaxScaler()
scaler.fit(train_df.values)

train_scaled = scaler.transform(train_df.values)
val_scaled   = scaler.transform(val_df.values)
test_scaled  = scaler.transform(test_df.values)

# Save scaler artifact
joblib.dump(scaler, "/kaggle/working/artifacts/scaler.pkl")
print("âœ… Saved: /kaggle/working/artifacts/scaler.pkl")






LOOKBACK = 24   # past 24 hours
HORIZON = 1     # predict next 1 hour

# Save config for Flask integration later
config = {
    "lookback": LOOKBACK,
    "horizon": HORIZON,
    "target_col": target_col,
    "selected_features": selected_features
}
with open("/kaggle/working/artifacts/config.json", "w") as f:
    json.dump(config, f, indent=2)
print("âœ… Saved: /kaggle/working/artifacts/config.json")


def make_windows(arr, lookback=24, horizon=1, target_index=-1):
    X, y = [], []
    for i in range(len(arr) - lookback - horizon + 1):
        X.append(arr[i:i+lookback])
        y.append(arr[i+lookback+horizon-1, target_index])
    return np.array(X, dtype=np.float32), np.array(y, dtype=np.float32).reshape(-1, 1)

# Target is last column in our scaled array (because we arranged selected_features + target)
target_index = len(all_cols) - 1

X_train, y_train = make_windows(train_scaled, LOOKBACK, HORIZON, target_index)
X_val, y_val     = make_windows(val_scaled, LOOKBACK, HORIZON, target_index)
X_test, y_test   = make_windows(test_scaled, LOOKBACK, HORIZON, target_index)

print("âœ… Windowed shapes:")
print("X_train:", X_train.shape, "y_train:", y_train.shape)
print("X_val  :", X_val.shape, "y_val  :", y_val.shape)
print("X_test :", X_test.shape, "y_test :", y_test.shape)






class SelfAttention(layers.Layer):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.att = layers.Attention()

    def call(self, inputs):
        # inputs: (batch, time, features)
        # self-attention: query=key=value=inputs
        context = self.att([inputs, inputs])
        return context

def build_cnn_bilstm_sa(input_shape):
    inp = keras.Input(shape=input_shape)

    x = layers.Conv1D(filters=64, kernel_size=3, padding="same", activation="relu")(inp)
    x = layers.MaxPooling1D(pool_size=2)(x)
    x = layers.Dropout(0.2)(x)

    x = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(x)
    x = layers.Dropout(0.2)(x)

    x = SelfAttention()(x)
    x = layers.GlobalAveragePooling1D()(x)

    x = layers.Dense(64, activation="relu")(x)
    x = layers.Dropout(0.2)(x)

    out = layers.Dense(1)(x)

    model = keras.Model(inp, out)
    model.compile(optimizer=keras.optimizers.Adam(1e-3), loss="mse", metrics=["mae"])
    return model

model = build_cnn_bilstm_sa(X_train.shape[1:])
model.summary()






# Callbacks (best practice)
ckpt_path = "/kaggle/working/artifacts/model_cnn_bilstm_sa.keras"

callbacks = [
    keras.callbacks.EarlyStopping(
        monitor="val_loss",
        patience=8,
        restore_best_weights=True
    ),
    keras.callbacks.ReduceLROnPlateau(
        monitor="val_loss",
        factor=0.5,
        patience=4,
        min_lr=1e-6,
        verbose=1
    ),
    keras.callbacks.ModelCheckpoint(
        filepath=ckpt_path,
        monitor="val_loss",
        save_best_only=True,
        verbose=1
    )
]

history = model.fit(
    X_train, y_train,
    validation_data=(X_val, y_val),
    epochs=50,
    batch_size=64,
    callbacks=callbacks,
    verbose=1
)

print("âœ… Training complete.")
print("âœ… Best model saved at:", ckpt_path)



plt.figure()
plt.plot(history.history["loss"], label="train_loss")
plt.plot(history.history["val_loss"], label="val_loss")
plt.xlabel("Epoch")
plt.ylabel("MSE Loss")
plt.legend()
plt.title("Training vs Validation Loss")
plt.show()






# Load best saved model (ensures we evaluate the best checkpoint)
best_model = keras.models.load_model(
    "/kaggle/working/artifacts/model_cnn_bilstm_sa.keras",
    custom_objects={"SelfAttention": SelfAttention}
)

# Predict
y_pred = best_model.predict(X_test, verbose=0)

# Metrics
rmse = math.sqrt(mean_squared_error(y_test, y_pred))
mae  = mean_absolute_error(y_test, y_pred)
r2   = r2_score(y_test, y_pred)

print("âœ… Test Metrics (Scaled target space):")
print("RMSE:", rmse)
print("MAE :", mae)
print("RÂ²  :", r2)



plt.figure()
plt.plot(y_test[:300], label="Actual")
plt.plot(y_pred[:300], label="Predicted")
plt.title("Actual vs Predicted (First 300 Test Points)")
plt.xlabel("Time step")
plt.ylabel("Scaled Global_active_power")
plt.legend()
plt.show()






# Helper: inverse transform only the target (last column)
def inverse_target(scaler, y_scaled, num_cols, target_index):
    dummy = np.zeros((len(y_scaled), num_cols), dtype=np.float32)
    dummy[:, target_index] = y_scaled.reshape(-1)
    inv = scaler.inverse_transform(dummy)
    return inv[:, target_index]

num_cols = len(all_cols)
t_idx = target_index

# Inverse transform y_test and y_pred to original kW units
y_test_kw = inverse_target(scaler, y_test, num_cols, t_idx)
y_pred_kw = inverse_target(scaler, y_pred, num_cols, t_idx)

# Metrics in original units
rmse_kw = math.sqrt(mean_squared_error(y_test_kw, y_pred_kw))
mae_kw  = mean_absolute_error(y_test_kw, y_pred_kw)
r2_kw   = r2_score(y_test_kw, y_pred_kw)

print("âœ… Test Metrics (Original kW units):")
print("RMSE (kW):", rmse_kw)
print("MAE  (kW):", mae_kw)
print("RÂ²       :", r2_kw)

# Save metrics
metrics = {
    "scaled": {"rmse": float(rmse), "mae": float(mae), "r2": float(r2)},
    "original_kw": {"rmse": float(rmse_kw), "mae": float(mae_kw), "r2": float(r2_kw)},
    "selected_features": selected_features,
    "lookback": LOOKBACK,
    "horizon": HORIZON
}

with open("/kaggle/working/artifacts/metrics_cnn_bilstm_sa.json", "w") as f:
    json.dump(metrics, f, indent=2)

print("âœ… Saved: /kaggle/working/artifacts/metrics_cnn_bilstm_sa.json")



plt.figure()
plt.plot(y_test_kw[:300], label="Actual (kW)")
plt.plot(y_pred_kw[:300], label="Predicted (kW)")
plt.title("Actual vs Predicted in Original Units (kW) â€” First 300")
plt.xlabel("Time step")
plt.ylabel("Global_active_power (kW)")
plt.legend()
plt.show()






def build_cnn_bigru_sa(input_shape):
    inp = keras.Input(shape=input_shape)

    x = layers.Conv1D(filters=64, kernel_size=3, padding="same", activation="relu")(inp)
    x = layers.MaxPooling1D(pool_size=2)(x)
    x = layers.Dropout(0.2)(x)

    x = layers.Bidirectional(layers.GRU(64, return_sequences=True))(x)
    x = layers.Dropout(0.2)(x)

    x = SelfAttention()(x)
    x = layers.GlobalAveragePooling1D()(x)

    x = layers.Dense(64, activation="relu")(x)
    x = layers.Dropout(0.2)(x)

    out = layers.Dense(1)(x)

    model = keras.Model(inp, out)
    model.compile(optimizer=keras.optimizers.Adam(1e-3), loss="mse", metrics=["mae"])
    return model

model_bigru = build_cnn_bigru_sa(X_train.shape[1:])
model_bigru.summary()






ckpt_path_bigru = "/kaggle/working/artifacts/model_cnn_bigru_sa.keras"

callbacks_bigru = [
    keras.callbacks.EarlyStopping(
        monitor="val_loss",
        patience=8,
        restore_best_weights=True
    ),
    keras.callbacks.ReduceLROnPlateau(
        monitor="val_loss",
        factor=0.5,
        patience=4,
        min_lr=1e-6,
        verbose=1
    ),
    keras.callbacks.ModelCheckpoint(
        filepath=ckpt_path_bigru,
        monitor="val_loss",
        save_best_only=True,
        verbose=1
    )
]

history_bigru = model_bigru.fit(
    X_train, y_train,
    validation_data=(X_val, y_val),
    epochs=50,
    batch_size=64,
    callbacks=callbacks_bigru,
    verbose=1
)

print("âœ… BiGRU Training complete.")
print("âœ… Best BiGRU model saved at:", ckpt_path_bigru)



plt.figure()
plt.plot(history_bigru.history["loss"], label="train_loss")
plt.plot(history_bigru.history["val_loss"], label="val_loss")
plt.xlabel("Epoch")
plt.ylabel("MSE Loss")
plt.legend()
plt.title("BiGRU â€” Training vs Validation Loss")
plt.show()






# Load best BiGRU model
best_bigru = keras.models.load_model(
    "/kaggle/working/artifacts/model_cnn_bigru_sa.keras",
    custom_objects={"SelfAttention": SelfAttention}
)

# Predict
y_pred_bigru = best_bigru.predict(X_test, verbose=0)

# Scaled metrics
rmse_bigru = math.sqrt(mean_squared_error(y_test, y_pred_bigru))
mae_bigru  = mean_absolute_error(y_test, y_pred_bigru)
r2_bigru   = r2_score(y_test, y_pred_bigru)

# In original kW
y_pred_bigru_kw = inverse_target(scaler, y_pred_bigru, num_cols, t_idx)

rmse_bigru_kw = math.sqrt(mean_squared_error(y_test_kw, y_pred_bigru_kw))
mae_bigru_kw  = mean_absolute_error(y_test_kw, y_pred_bigru_kw)
r2_bigru_kw   = r2_score(y_test_kw, y_pred_bigru_kw)

# Comparison
comp = pd.DataFrame({
    "Model": ["CNN-BiLSTM-SA", "CNN-BiGRU-SA"],
    "RMSE_scaled": [rmse, rmse_bigru],
    "MAE_scaled":  [mae, mae_bigru],
    "R2_scaled":   [r2, r2_bigru],
    "RMSE_kW":     [rmse_kw, rmse_bigru_kw],
    "MAE_kW":      [mae_kw, mae_bigru_kw],
    "R2_kW":       [r2_kw, r2_bigru_kw]
})

display(comp)

# Save BiGRU metrics too
metrics_bigru = {
    "scaled": {"rmse": float(rmse_bigru), "mae": float(mae_bigru), "r2": float(r2_bigru)},
    "original_kw": {"rmse": float(rmse_bigru_kw), "mae": float(mae_bigru_kw), "r2": float(r2_bigru_kw)},
    "selected_features": selected_features,
    "lookback": LOOKBACK,
    "horizon": HORIZON
}

with open("/kaggle/working/artifacts/metrics_cnn_bigru_sa.json", "w") as f:
    json.dump(metrics_bigru, f, indent=2)

print("âœ… Saved: /kaggle/working/artifacts/metrics_cnn_bigru_sa.json")






import shutil, glob

ART = "/kaggle/working/artifacts"
os.makedirs(ART, exist_ok=True)

# Copy best model as final alias (easier for Flask)
src_model = f"{ART}/model_cnn_bilstm_sa.keras"
final_model = f"{ART}/final_model.keras"
shutil.copyfile(src_model, final_model)

# Copy BiLSTM metrics as final metrics
src_metrics = f"{ART}/metrics_cnn_bilstm_sa.json"
final_metrics = f"{ART}/metrics_final.json"
shutil.copyfile(src_metrics, final_metrics)

print("âœ… Final artifacts created:")
print(" -", final_model)
print(" -", final_metrics)

# List what we have
print("\nðŸ“ artifacts folder contents:")
for p in sorted(glob.glob(f"{ART}/*")):
    print(" -", os.path.basename(p))






import zipfile

zip_path = "/kaggle/working/electricity_forecasting_artifacts.zip"
files_to_zip = [
    "/kaggle/working/artifacts/final_model.keras",
    "/kaggle/working/artifacts/scaler.pkl",
    "/kaggle/working/artifacts/selected_features.json",
    "/kaggle/working/artifacts/config.json",
    "/kaggle/working/artifacts/metrics_final.json",
]

with zipfile.ZipFile(zip_path, "w", compression=zipfile.ZIP_DEFLATED) as z:
    for fpath in files_to_zip:
        if os.path.exists(fpath):
            z.write(fpath, arcname=os.path.basename(fpath))
        else:
            print("âš ï¸ Missing (not zipped):", fpath)

print("âœ… ZIP created:", zip_path)
print("âž¡ï¸ Kaggle: Right sidebar â†’ Output â†’ Download electricity_forecasting_artifacts.zip")






# Load final artifacts
scaler_loaded = joblib.load("/kaggle/working/artifacts/scaler.pkl")
with open("/kaggle/working/artifacts/selected_features.json") as f:
    feats = json.load(f)
with open("/kaggle/working/artifacts/config.json") as f:
    cfg = json.load(f)

lookback = int(cfg["lookback"])
target_col = cfg["target_col"]
all_cols_live = feats + [target_col]

# Load final model
final_loaded = keras.models.load_model(
    "/kaggle/working/artifacts/final_model.keras",
    custom_objects={"SelfAttention": SelfAttention}
)

# Prepare last window from df_hourly (unscaled)
last_block = df_hourly[all_cols_live].iloc[-lookback:].values  # (24, 6)
last_scaled = scaler_loaded.transform(last_block)              # (24, 6)

X_live = last_scaled.reshape(1, lookback, last_scaled.shape[1])  # (1,24,6)

pred_scaled = final_loaded.predict(X_live, verbose=0).reshape(-1, 1)

# Inverse-transform to kW using dummy array trick
num_cols_live = len(all_cols_live)
t_idx_live = num_cols_live - 1

pred_kw = inverse_target(scaler_loaded, pred_scaled, num_cols_live, t_idx_live)[0]

print("âœ… Flask-style inference test passed.")
print("Predicted next-hour Global_active_power (kW):", float(pred_kw))

